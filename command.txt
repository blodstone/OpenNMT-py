########
Baseline: See
########

python preprocess.py -train_src data/bbc-split/src.train.token.lower_no_tag -train_tgt data/bbc-split/tgt.txt.train.lower -valid_src data/bbc-split/src.validation.token.lower_no_tag -valid_tgt data/bbc-split/tgt.txt.validation.lower -save_data data/preprocess-bbc/bbc_no_tag -src_seq_length 10000 -tgt_seq_length 10000 -src_seq_length_trunc 400 -tgt_seq_length_trunc 100 -dynamic_dict -share_vocab -shard_size 100000

CUDA_VISIBLE_DEVICES=0 python train.py -save_model models/opennmt1024-bbc-debug -data data/preprocess-bbc/bbc -copy_attn -global_attention mlp -word_vec_size 256 -rnn_size 1024 -layers 2 -encoder_type brnn -train_steps 50 -max_grad_norm 2 -dropout 0. -batch_size 16 -valid_batch_size 16 -optim adagrad -learning_rate 0.15 -adagrad_accumulator_init 0.1 -reuse_copy_attn -copy_loss_by_seqlength -bridge -seed 777 -world_size 1 -gpu_ranks 0 -log_file out/train_opennmt1024_debug_01.log

CUDA_VISIBLE_DEVICES=0 python translate.py -gpu 0 -batch_size 40 -beam_size 10 -model models/save_models/opennmt1024-bbc_step_170000.pt -src data/bbc-split/src.test.token.lower -output out/opennmt1024_bbc_test.out -min_length 35 -verbose -stepwise_penalty -coverage_penalty summary -beta 5 -length_penalty wu -alpha 0.9 -block_ngram_repeat 3 -ignore_when_blocking "." "<sos>" "<eos>"

python parse_rouge.py -c ../out/opennmt1024_bbc_test.out -r ../data/bbc-split/tgt.txt.test -output_dir ../out -output_name opennmt1024_test

python test_rouge.py -c out/opennmt1024_test.c.txt -r out/opennmt1024_test.r.txt

Validation
OpenNMT1024 lower ROUGE(1/2/3/L/SU4): 27.88/7.76/2.85/20.96/7.96
OpenNMT1024 lower no tag ROUGE(1/2/3/L/SU4): 22.06/5.76/2.14/17.07/6.01

Test

OpenNMT512 ROUGE(1/2/3/L/SU4): 25.05/5.50/1.72/18.58/6.44
Embedded: 29.19/8.32/3.15/22.30/8.92
######
Our: Topic attention
######
python topic_matrix_to_tensor.py -emb_file ../data/bbc-split/topic_matrix.lda -output_file ../data/bbc-split/topic_matrix.tensor -dict_file ../data/bbc-split.vocab.pt

python preprocess.py -train_src data/bbc-split/src.train.token.lower -train_topic data/bbc-split/src.lda.train -train_lemma data/bbc-split/src.train.lemma.lower -train_tgt data/bbc-split/tgt.txt.train.lower -valid_src data/bbc-split/src.validation.token.lower -valid_tgt data/bbc-split/tgt.txt.validation.lower -valid_topic data/bbc-split/src.lda.validation -valid_lemma data/bbc-split/src.validation.lemma.lower -save_data data/preprocess-bbc-split/bbc_topic -src_seq_length 10000 -tgt_seq_length 10000 -src_seq_length_trunc 400 -tgt_seq_length_trunc 100 -dynamic_dict -share_vocab -shard_size 100000

CUDA_VISIBLE_DEVICES=0 python train.py -save_model models/topic1024+dot+joint+weighted+sparsemax+512 -data data/preprocess-bbc-split/bbc_topic -copy_attn -global_attention mlp -word_vec_size 256 -rnn_size 1024 -layers 2 -encoder_type brnn -train_steps 250000 -max_grad_norm 2 -dropout 0. -batch_size 16 -valid_batch_size 16 -optim adagrad -learning_rate 0.15 -adagrad_accumulator_init 0.1 -seed 777 -topic_matrix data/preprocess-topic-bbc/t-512-shashi/topic_matrix.tensor -bridge -world_size 0 -gpu_ranks 0 -reuse_copy_attn -copy_loss_by_seqlength -log_file out/topic1024+dot+joint+weighted+sparsemax+512.log -joint_attn_mode co_attention -pooling column -topic_attn dot -topic_attn_function sparsemax

CUDA_VISIBLE_DEVICES=0 python translate.py -gpu 0 -batch_size 50 -beam_size 10 -model models/save_models/topic1024+embedded+512_1_step_165000.pt -src data/bbc-split/src.validation.token.lower -lemma data/bbc-split/src.validation.lemma.lower -topic_matrix data/preprocess-topic-bbc/t-512-shashi/topic_matrix.tensor -output out/topic1024+embedded+512_1_step_165000.out -min_length 35 -verbose -stepwise_penalty -coverage_penalty summary -beta 5 -length_penalty wu -alpha 0.9 -block_ngram_repeat 3 -ignore_when_blocking "." "</t>" "<t>" "<sos>" "<eos>" -joint_attn_mode embedded

python parse_rouge.py -c ../testout/topic1024_debug.out -r ../data/bbc-split/src.validation.token.lower.small -output_dir ../out -output_name topic1024_debug

python test_rouge.py -c out/topic1024_debug.c.txt -r out/topic1024_debug.r.txt
[2019-05-06 21:31:11,078 INFO] encoder: 7322624
[2019-05-06 21:31:11,078 INFO] decoder: 35990868
[2019-05-06 21:31:11,078 INFO] * number of pa
rameters: 43313492

topic1024-bbc+tf+075ah ROUGE(1/2/3/L/SU4): 26.76/7.11/2.55/20.13/7.39
topic1024-bbc+tf+05ah ROUGE(1/2/3/L/SU4): 24.44/6.35/2.38/18.71/6.49
topic1024-bbc+tf+025ah ROUGE(1/2/3/L/SU4): 20.67/3.83/1.24/15.71/4.81

OpenNMT1024 ROUGE(1/2/3/L/SU4) 35 length: 27.74/7.75/2.88/20.85/7.88
topic1024+embedded 25 length replace_unk ROUGE(1/2/3/L/SU4): 29.24/8.25/3.06/22.28/8.98
topic1024+embedded 25 length ROUGE(1/2/3/L/SU4): 28.61/8.13/3.01/21.85/8.73
topic1024+embedded 35 length ROUGE(1/2/3/L/SU4): 27.93/7.78/2.84/20.90/8.01

topic1024+tf+multihead+100 ROUGE(1/2/3/L/SU4): 26.76/6.97/2.44/20.02/7.33
topic1024+tf+multihead+1512 ROUGE(1/2/3/L/SU4): 26.97/7.03/2.47/20.18/7.43



Debug
New approach
512 [2019-07-08 15:32:57,062 INFO] >> ROUGE(1/2/3/L/SU4): 15.14/2.08/0.00/12.76/2.25
3 [2019-07-08 15:33:51,614 INFO] >> ROUGE(1/2/3/L/SU4): 24.82/2.08/0.00/16.82/4.97


Pre Experiment:
512 [2019-07-06 22:46:30,497 INFO] >> ROUGE(1/2/3/L/SU4): 25.23/5.13/1.08/17.41/5.90
3 [2019-07-06 22:51:24,932 INFO] >> ROUGE(1/2/3/L/SU4): 25.99/4.13/0.90/16.18/5.69

New approach
Embedded Full [2019-07-21 22:22:27,043 INFO] >> ROUGE(1/2/3/L/SU4): 29.74/8.58/3.29/22.81/9.30
Embedded 170k [2019-07-21 21:20:23,164 INFO] >> ROUGE(1/2/3/L/SU4): 28.88/6.67/1.94/21.11/8.09
Embedded 165k [2019-07-21 16:41:06,664 INFO] >> ROUGE(1/2/3/L/SU4): 24.47/5.62/2.04/17.77/6.27
Embedded 150k [2019-07-21 16:56:36,381 INFO] >> ROUGE(1/2/3/L/SU4): 25.93/6.14/2.10/19.24/6.92
512 [2019-07-08 15:26:56,351 INFO] >> ROUGE(1/2/3/L/SU4): 28.32/7.50/1.80/21.02/8.24
3 [2019-07-08 15:10:40,623 INFO] >> ROUGE(1/2/3/L/SU4): 26.73/6.92/1.33/19.13/7.35
OpenNMT ROUGE(1/2/3/L/SU4): 24.89/6.20/2.68/19.18/6.52

python train.py -save_model models/topic_debug -data data/preprocess-bbc-split/bbc_topic -copy_attn -global_attention mlp -word_vec_size 128 -rnn_size 256 -layers 1 -encoder_type brnn -train_steps 1 -max_grad_norm 2 -dropout 0. -batch_size 1 -valid_batch_size 1 -optim adagrad -learning_rate 0.15 -adagrad_accumulator_init 0.1 -seed 777 -topic_matrix data/lda-train-document-lemma-topic-512-iter-1000/topic_matrix.tensor -bridge -reuse_copy_attn -copy_loss_by_seqlength -log_file out/topic_debug.log -joint_attn_mode co-attention -pooling column -topic_attn dot -weighted_co_attn -topic_attn_function sparsemax

######
Our: OpenNMT and Topic attention using debug dataset
######
--
OpenNMT
--
CUDA_VISIBLE_DEVICES=0 python translate.py -gpu 0 -batch_size 20 -beam_size 10 -model models/save_models/topic1024+tf+05ah_step_170000.pt -src data/bbc-split/src.debug.token.lower -output out/opennmt1024-bbc_debug.out -min_length 35 -verbose -stepwise_penalty -coverage_penalty summary -beta 5 -length_penalty wu -alpha 0.9 -block_ngram_repeat 3 -ignore_when_blocking "." "</t>" "<t>" "<sos>" "<eos>"
--
Topic Attention
--
CUDA_VISIBLE_DEVICES=0 python translate.py -gpu 0 -batch_size 20 -beam_size 5 -model models/save_models/topic1024+tf+05ah_step_170000.pt -src data/bbc-split/src.debug.token.lower -lemma data/bbc-split/src.debug.lemma.lower -topic_matrix data/bbc-split/topic_matrix.tensor -output out/topic1024-bbc+ss+avg_debug.out -min_length 35 -verbose -stepwise_penalty -coverage_penalty summary -beta 5 -length_penalty wu -alpha 0.9 -block_ngram_repeat 3 -ignore_when_blocking "." "</t>" "<t>" "<sos>" "<eos>" -attn_debug

CUDA_VISIBLE_DEVICES=0 python train.py -save_model models/topic1024+dot+joint+sparsemax+512 -data data/preprocess-bbc-split/bbc_topic -copy_attn -global_attention mlp -word_vec_size 256 -rnn_size 1024 -layers 2 -encoder_type brnn -train_steps 250000 -max_grad_norm 2 -dropout 0. -batch_size 12 -valid_batch_size 12 -optim adagrad -learning_rate 0.15 -adagrad_accumulator_init 0.1 -seed 777 -topic_matrix data/preprocess-topic-bbc/t-512-shashi/topic_matrix.tensor -bridge -world_size 0 -gpu_ranks 0 -reuse_copy_attn -copy_loss_by_seqlength -log_file out/topic1024+dot+joint+sparsemax+512.log -joint_attn_mode co_attention -pooling column -topic_attn dot -topic_attn_function sparsemax -train_from models/topic1024+dot+joint+sparsemax+512_step_5000.pt
