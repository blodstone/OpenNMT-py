########
Baseline: See
########

python preprocess.py -train_src data/bbc-split/src.train.token.lower_no_tag -train_tgt data/bbc-split/tgt.txt.train.lower -valid_src data/bbc-split/src.validation.token.lower_no_tag -valid_tgt data/bbc-split/tgt.txt.validation.lower -save_data data/preprocess-bbc/bbc_no_tag -src_seq_length 10000 -tgt_seq_length 10000 -src_seq_length_trunc 400 -tgt_seq_length_trunc 100 -dynamic_dict -share_vocab -shard_size 100000

CUDA_VISIBLE_DEVICES=0 python train.py -save_model models/opennmt1024-bbc-no-tag -data data/preprocess-bbc/bbc_no_tag -copy_attn -global_attention mlp -word_vec_size 256 -rnn_size 1024 -layers 2 -encoder_type brnn -train_steps 200000 -max_grad_norm 2 -dropout 0. -batch_size 16 -valid_batch_size 16 -optim adagrad -learning_rate 0.15 -adagrad_accumulator_init 0.1 -reuse_copy_attn -copy_loss_by_seqlength -bridge -seed 777 -world_size 1 -gpu_ranks 0 -log_file out/train_opennmt1024_no_tag_01.log

CUDA_VISIBLE_DEVICES=0 python translate.py -gpu 0 -batch_size 40 -beam_size 10 -model models/save_models/opennmt1024-bbc_step_170000.pt -src data/bbc-split/src.test.token.lower -output out/opennmt1024_bbc_test.out -min_length 35 -verbose -stepwise_penalty -coverage_penalty summary -beta 5 -length_penalty wu -alpha 0.9 -block_ngram_repeat 3 -ignore_when_blocking "." "<sos>" "<eos>"

python parse_rouge.py -c ../out/opennmt1024_bbc_test.out -r ../data/bbc-split/tgt.txt.test -output_dir ../out -output_name opennmt1024_test

python test_rouge.py -c out/opennmt1024_test.c.txt -r out/opennmt1024_test.r.txt

Validation
OpenNMT1024 lower ROUGE(1/2/3/L/SU4): 27.88/7.76/2.85/20.96/7.96
OpenNMT1024 lower no tag ROUGE(1/2/3/L/SU4): 22.06/5.76/2.14/17.07/6.01

Test
OpenNMT512 ROUGE(1/2/3/L/SU4): 25.05/5.50/1.72/18.58/6.44
OpenNMT1024 ROUGE(1/2/3/L/SU4): 27.74/7.75/2.88/20.85/7.88

######
Our: Topic attention
######
python topic_matrix_to_tensor.py -emb_file ../data/bbc-split/topic_matrix.lda -output_file ../data/bbc-split/topic_matrix.tensor -dict_file ../data/bbc-split.vocab.pt

python preprocess.py -train_src data/bbc-split/src.train.token.lower -train_topic data/bbc-split/src.lda.train -train_lemma data/bbc-split/src.train.lemma.lower -train_tgt data/bbc-split/tgt.txt.train.lower -valid_src data/bbc-split/src.validation.token.lower -valid_tgt data/bbc-split/tgt.txt.validation.lower -valid_topic data/bbc-split/src.lda.validation -valid_lemma data/bbc-split/src.validation.lemma.lower -save_data data/preprocess-bbc-split/bbc_topic -src_seq_length 10000 -tgt_seq_length 10000 -src_seq_length_trunc 400 -tgt_seq_length_trunc 100 -dynamic_dict -share_vocab -shard_size 100000

CUDA_VISIBLE_DEVICES=0 python train.py -theta 0.75 -save_model models/topic1024+tf+075ah+100 -data data/preprocess-bbc-split/bbc_topic -copy_attn -global_attention mlp -word_vec_size 256 -rnn_size 1024 -layers 2 -encoder_type brnn -train_steps 200000 -max_grad_norm 2 -dropout 0. -batch_size 16 -valid_batch_size 16 -optim adagrad -learning_rate 0.15 -adagrad_accumulator_init 0.1 -seed 777 -topic_matrix data/preprocess-topic-bbc/t-100-big/topic_matrix.tensor -bridge -world_size 0 -gpu_ranks 0 -reuse_copy_attn -copy_loss_by_seqlength -log_file out/topic1024+tf+075ah+100.log

CUDA_VISIBLE_DEVICES=0 python translate.py -gpu 0 -batch_size 50 -beam_size 10 -model models/save_models/topic1024+tf+025ah_step_180000.pt -src data/bbc-split/src.validation.token.lower -lemma data/bbc-split/src.validation.lemma.lower -topic_matrix data/bbc-split/topic_matrix.tensor -theta 0.5 -output out/topic1024+tf+025ah.out -min_length 35 -verbose -stepwise_penalty -coverage_penalty summary -beta 5 -length_penalty wu -alpha 0.9 -block_ngram_repeat 3 -ignore_when_blocking "." "</t>" "<t>" "<sos>" "<eos>"

python parse_rouge.py -c ../out/topic1024+tf+025ah.out -r ../data/bbc-split/tgt.txt.validation.lower -output_dir ../out -output_name topic1024+tf+025ah

python test_rouge.py -c out/topic1024+tf+025ah.c.txt -r out/topic1024+tf+025ah.r.txt
[2019-05-06 21:31:11,078 INFO] encoder: 7322624
[2019-05-06 21:31:11,078 INFO] decoder: 35990868
[2019-05-06 21:31:11,078 INFO] * number of pa
rameters: 43313492

topic1024-bbc+tf+075ah ROUGE(1/2/3/L/SU4): 26.76/7.11/2.55/20.13/7.39
topic1024-bbc+tf+05ah ROUGE(1/2/3/L/SU4): 24.44/6.35/2.38/18.71/6.49
topic1024-bbc+tf+025ah ROUGE(1/2/3/L/SU4): 20.67/3.83/1.24/15.71/4.81

######
Our: OpenNMT and Topic attention using debug dataset
######
--
OpenNMT
--
CUDA_VISIBLE_DEVICES=0 python translate.py -gpu 0 -batch_size 20 -beam_size 10 -model models/save_models/topic1024+tf+05ah_step_170000.pt -src data/bbc-split/src.debug.token.lower -output out/opennmt1024-bbc_debug.out -min_length 35 -verbose -stepwise_penalty -coverage_penalty summary -beta 5 -length_penalty wu -alpha 0.9 -block_ngram_repeat 3 -ignore_when_blocking "." "</t>" "<t>" "<sos>" "<eos>"
--
Topic Attention
--
CUDA_VISIBLE_DEVICES=0 python translate.py -gpu 0 -batch_size 20 -beam_size 5 -model models/save_models/topic1024+tf+05ah_step_170000.pt -src data/bbc-split/src.debug.token.lower -lemma data/bbc-split/src.debug.lemma.lower -topic_matrix data/bbc-split/topic_matrix.tensor -output out/topic1024-bbc+ss+avg_debug.out -min_length 35 -verbose -stepwise_penalty -coverage_penalty summary -beta 5 -length_penalty wu -alpha 0.9 -block_ngram_repeat 3 -ignore_when_blocking "." "</t>" "<t>" "<sos>" "<eos>" -attn_debug
