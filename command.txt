

Baseline: See

python preprocess.py -train_src data/bbc-split/src.train.token.lower -train_tgt data/bbc-split/tgt.txt.train.lower -valid_src data/bbc-split/src.validation.token.lower -valid_tgt data/bbc-split/tgt.txt.validation.lower -save_data data/preprocess-bbc/bbc -src_seq_length 10000 -tgt_seq_length 10000 -src_seq_length_trunc 400 -tgt_seq_length_trunc 100 -dynamic_dict -share_vocab -shard_size 100000

CUDA_VISIBLE_DEVICES=0 python train.py -save_model models/opennmt1024-bbc -data data/preprocess-bbc/bbc -copy_attn -global_attention mlp -word_vec_size 256 -rnn_size 1024 -layers 2 -encoder_type brnn -train_steps 200000 -max_grad_norm 2 -dropout 0. -batch_size 16 -valid_batch_size 16 -optim adagrad -learning_rate 0.15 -adagrad_accumulator_init 0.1 -reuse_copy_attn -copy_loss_by_seqlength -bridge -seed 777 -world_size 1 -gpu_ranks 0 -log_file testout/train_opennmt1024_01.log

CUDA_VISIBLE_DEVICES=0 python translate.py -gpu 0 -batch_size 40 -beam_size 10 -model models/save_models/opennmt1024-bbc_step_170000.pt -src data/bbc-split/src.validation.token.lower -output out/opennmt1024_bbc_validation.out -min_length 35 -verbose -stepwise_penalty -coverage_penalty summary -beta 5 -length_penalty wu -alpha 0.9 -block_ngram_repeat 3 -ignore_when_blocking "." "</t>" "<t>" -log_file out/translate_opennmt1024_val_01.log

python parse_rouge.py -c ../out/opennmt1024_bbc_validation.out -r ../data/bbc-split/tgt.txt.validation -output_dir ../out -output_name opennmt1024

python test_rouge.py -c out/opennmt1024.c.txt -r out/opennmt1024.r.txt

OpenNMT1024 ROUGE(1/2/3/L/SU4): 27.88/7.76/2.85/20.96/7.96
OpenNMT512 ROUGE(1/2/3/L/SU4): 25.05/5.50/1.72/18.58/6.44

Our: Topic attention
python topic_matrix_to_tensor.py -emb_file ../data/bbc-split/topic_matrix.lda -output_file ../data/bbc-split/topic_matrix.tensor -dict_file ../data/bbc-split.vocab.pt

python preprocess.py -train_src data/bbc-split/src.train.token -train_topic data/bbc-split/src.lda.train -train_lemma data/bbc-split/src.train.lemma -train_tgt data/bbc-split/tgt.txt.train -valid_src data/bbc-split/src.validation.token -valid_tgt data/bbc-split/tgt.txt.validation -valid_topic data/bbc-split/src.lda.validation -valid_lemma data/bbc-split/src.validation.lemma -save_data data/topic-bbc-split -src_seq_length 10000 -tgt_seq_length 10000 -src_seq_length_trunc 400 -tgt_seq_length_trunc 100 -dynamic_dict -share_vocab -shard_size 100000

CUDA_VISIBLE_DEVICES=0 python train.py -save_model models/bbc-split -data data/bbc-split -global_attention mlp -word_vec_size 128 -rnn_size 512 -layers 1 -encoder_type brnn -train_steps 200000 --valid_steps 10000 -max_grad_norm 2 -dropout 0. -batch_size 8 -valid_batch_size 8 -optim adagrad -learning_rate 0.15 -adagrad_accumulator_init 0.1 -seed 777 -topic_attn -topic_matrix data/bbc-split/topic_matrix.tensor -lemma-align data/bbc-split/src.train.pair -bridge -world_size 0 -gpu_ranks 0 -model_dtype fp16 -log_file testout/train_topic_01.log -train_from models/bbc-split_step_10000.pt

CUDA_VISIBLE_DEVICES=0 python translate.py -gpu 0 -batch_size 20 -beam_size 10 -model models/save_models/topic-bbc-512_step_130000.pt -src data/bbc-split/src.test.token -lemma data/bbc-split/src.test.lemma -lemma-align data/bbc-split/src.train.pair -topic_matrix data/bbc-split/topic_matrix.tensor -output testout/topic_bbc.out -min_length 35 -verbose -stepwise_penalty -coverage_penalty summary -beta 5 -length_penalty wu -alpha 0.9 -block_ngram_repeat 3 -ignore_when_blocking "." "</t>" "<t>" "<sos>" "<eos>" --report_rouge --replace_unk -log_file testout/translate_topic_01.log

python parse_rouge.py -c ../testout/bbc_test.out -r ../data/bbc-split/tgt.txt.test -output_dir ../testout -output_name topic512+es+cl

python test_rouge.py -c testout/topic512+es+cl.c.txt -r testout/topic512+es+cl.r.txt
[2019-05-06 21:31:11,078 INFO] encoder: 7322624
[2019-05-06 21:31:11,078 INFO] decoder: 35990868
[2019-05-06 21:31:11,078 INFO] * number of parameters: 43313492

ROUGE(1/2/3/L/SU4)
27.24/8.01
